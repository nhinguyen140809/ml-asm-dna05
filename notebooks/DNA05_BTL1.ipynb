{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Giới thiệu**\n",
        "\n",
        "Trong bài này, chúng ta xây dựng một **pipeline học máy hoàn chỉnh** cho bài toán phân loại bệnh tim.  \n",
        "Pipeline bao gồm các bước: EDA, tiền xử lý dữ liệu, trích xuất & lựa chọn đặc trưng, huấn luyện nhiều mô hình, đánh giá theo các chỉ số (Accuracy, Precision, Recall, F1-score), và trực quan hóa kết quả.  \n",
        "\n",
        "Bộ dữ liệu sử dụng được lấy từ Kaggle:  \n",
        "- [Heart Disease Dataset – oktayrdeki](https://www.kaggle.com/datasets/oktayrdeki/heart-disease)  \n",
        "\n",
        "Toàn bộ nội dung cùng các Assignment khác trong môn **Machine Learning – HK251** được tổng hợp tại:  \n",
        "- [Trang tổng hợp Machine Learning – Nhóm DNA05](https://nhinguyen140809.github.io/ml-asm-dna05/)  \n",
        "\n",
        "Dưới đây là phần cài đặt và import các thư viện cần thiết để bắt đầu thực nghiệm.\n"
      ],
      "metadata": {
        "id": "i3BwEuL2sW5Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KIvgHIhtIsM"
      },
      "source": [
        "# **Cài đặt và Import Thư viện**\n",
        "Trong bước này, chúng ta sẽ:\n",
        "- Cài đặt các thư viện cần thiết cho pipeline học máy.\n",
        "- Import đầy đủ các module phục vụ cho **EDA, tiền xử lý, trích xuất đặc trưng, huấn luyện và đánh giá mô hình**.\n",
        "- Thiết lập một số cấu hình cơ bản (ví dụ: tắt cảnh báo không quan trọng để log gọn gàng hơn).\n",
        "\n",
        "Lưu ý:\n",
        "- Người dùng có thể thêm/bớt thư viện nếu mở rộng pipeline (ví dụ: deep learning)\n",
        "- Nếu notebook đã có sẵn môi trường, có thể bỏ qua lệnh `!pip install`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmMLDzTpcson"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cài đặt thư viện cần thiết\n",
        "# ============================================================\n",
        "!pip install kagglehub plotly scikit-learn imbalanced-learn -q\n",
        "\n",
        "# ============================================================\n",
        "# Import các thư viện\n",
        "# ============================================================\n",
        "\n",
        "# Xử lý hệ thống & dữ liệu\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Trực quan hóa\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tiền xử lý dữ liệu\n",
        "from sklearn.preprocessing import (\n",
        "    OrdinalEncoder, LabelEncoder,\n",
        "    StandardScaler, MinMaxScaler\n",
        ")\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Mô hình học máy\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Pipeline & Feature Extraction\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Đánh giá mô hình\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "# Xử lý mất cân bằng dữ liệu\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Hỗ trợ hiển thị\n",
        "from IPython.display import display\n",
        "\n",
        "# Kaggle dataset downloader\n",
        "import kagglehub\n",
        "\n",
        "# Thiết lập hiển thị & warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.float_format\", \"{:.3f}\".format)\n",
        "\n",
        "print(\"Import và cài đặt thư viện thành công!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23_NPIcQyKU1"
      },
      "source": [
        "# **Tải Dataset**\n",
        "\n",
        "Trong bước này, chúng ta sẽ:\n",
        "- Tải **Heart Disease Dataset** từ Kaggle thông qua `kagglehub`.\n",
        "- Tự động tìm file `.csv` trong thư mục tải về và đọc vào `pandas.DataFrame`.\n",
        "- Thiết lập cách xử lý giá trị thiếu (`na_values`) để dữ liệu được chuẩn hóa ngay từ đầu.\n",
        "\n",
        "Cấu hình có thể thay đổi:\n",
        "- `DATASET_NAME`: Tên dataset trên Kaggle (có thể thay đổi sang dataset khác).\n",
        "- `FILE_EXTENSION`: Phần mở rộng của file dữ liệu cần đọc (mặc định `.csv`).\n",
        "- `NA_VALUES`: Các giá trị coi là missing (có thể thêm `\"NA\"`, `\"?\"` nếu dataset khác)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFUOfeTiyKdi"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cấu hình tải dataset\n",
        "# ============================================================\n",
        "CONFIG = {\n",
        "    \"DATASET_NAME\": \"oktayrdeki/heart-disease\",  # Thay đổi nếu muốn dùng dataset khác\n",
        "    \"FILE_EXTENSION\": \".csv\",                    # Có thể đổi sang .xlsx nếu cần\n",
        "    \"NA_VALUES\": [\"\", np.nan]                    # Các giá trị coi là missing\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# Hàm tải và đọc dataset\n",
        "# ============================================================\n",
        "def download_dataset(dataset_name: str, file_extension: str = \".csv\") -> str:\n",
        "    \"\"\"\n",
        "    Tải dataset từ Kaggle và trả về đường dẫn file dữ liệu.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): Tên dataset trên Kaggle (ví dụ: \"oktayrdeki/heart-disease\")\n",
        "        file_extension (str): Loại file cần lấy (.csv, .xlsx,...)\n",
        "\n",
        "    Returns:\n",
        "        str: Đường dẫn file dữ liệu\n",
        "    \"\"\"\n",
        "    path = kagglehub.dataset_download(dataset_name)\n",
        "    print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "    for f in os.listdir(path):\n",
        "        if f.endswith(file_extension):\n",
        "            return os.path.join(path, f)\n",
        "\n",
        "    raise FileNotFoundError(f\"Không tìm thấy file {file_extension} trong dataset!\")\n",
        "\n",
        "# ============================================================\n",
        "# Đọc dữ liệu vào DataFrame\n",
        "# ============================================================\n",
        "csv_path = download_dataset(CONFIG[\"DATASET_NAME\"], CONFIG[\"FILE_EXTENSION\"])\n",
        "\n",
        "df = pd.read_csv(\n",
        "    csv_path,\n",
        "    keep_default_na=False,\n",
        "    na_values=CONFIG[\"NA_VALUES\"]\n",
        ")\n",
        "\n",
        "print(\"Dataset loaded thành công!\")\n",
        "display(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk3zUILTtWhB"
      },
      "source": [
        "# **1. EDA – Exploratory Data Analysis**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrAOasXbl1AA"
      },
      "source": [
        "## 1.1. Tổng quan dữ liệu  \n",
        "\n",
        "Trong bước này, chúng ta sẽ:  \n",
        "- Hiển thị **5 dòng đầu tiên** để có cái nhìn nhanh về cấu trúc dữ liệu.  \n",
        "- Tạo bảng **tổng quan dataset**: kiểu dữ liệu, số lượng non-null, số lượng null, số giá trị unique.  \n",
        "- Thống kê **missing values** cho từng cột.  \n",
        "- Sinh **summary statistics** riêng cho **numeric** và **categorical** columns.  \n",
        "\n",
        "Cấu hình có thể thay đổi:\n",
        "- `N_HEAD`: số dòng muốn hiển thị ở đầu dataset.  \n",
        "- Có thể bật/tắt hiển thị từng bảng bằng các flag `SHOW_NUMERIC_SUMMARY`, `SHOW_CATEGORICAL_SUMMARY`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj-oLIpQctLH"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cấu hình EDA\n",
        "# ============================================================\n",
        "EDA_CONFIG = {\n",
        "    \"N_HEAD\": 5,                     # Số dòng đầu hiển thị\n",
        "    \"SHOW_NUMERIC_SUMMARY\": True,    # Bật/tắt thống kê numeric\n",
        "    \"SHOW_CATEGORICAL_SUMMARY\": True # Bật/tắt thống kê categorical\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# Hiển thị 5 dòng đầu tiên\n",
        "# ============================================================\n",
        "display(df.head(EDA_CONFIG[\"N_HEAD\"]).style.set_caption(f\"{EDA_CONFIG['N_HEAD']} dòng đầu tiên\"))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# Tổng quan dataset\n",
        "# ============================================================\n",
        "overview = pd.DataFrame({\n",
        "    \"Dtype\": df.dtypes,\n",
        "    \"Non-Null Count\": df.notnull().sum(),\n",
        "    \"Null Count\": df.isnull().sum(),\n",
        "    \"Unique Values\": df.nunique()\n",
        "})\n",
        "display(overview.style.set_caption(\"Tổng quan Dataset\"))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# Thống kê Missing Values\n",
        "# ============================================================\n",
        "missing = df.isnull().sum().sort_values(ascending=False)\n",
        "if missing.sum() > 0:\n",
        "    display(missing.to_frame(\"Số lượng missing\").style.set_caption(\"Thống kê Missing Values\"))\n",
        "else:\n",
        "    print(\"Không có missing values trong dataset.\")\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# Numeric summary\n",
        "# ============================================================\n",
        "if EDA_CONFIG[\"SHOW_NUMERIC_SUMMARY\"]:\n",
        "    num_summary = df.describe().T\n",
        "    if not num_summary.empty:\n",
        "        display(num_summary.style.set_caption(\"Thống kê Numeric Columns\"))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# Categorical summary\n",
        "# ============================================================\n",
        "if EDA_CONFIG[\"SHOW_CATEGORICAL_SUMMARY\"]:\n",
        "    cat_summary = df.describe(include=['object', 'category']).T\n",
        "    if not cat_summary.empty:\n",
        "        display(cat_summary.style.set_caption(\"Thống kê Categorical Columns\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRuJOxN1l7wi"
      },
      "source": [
        "## **1.2. Thống kê tần suất nhãn**\n",
        "\n",
        "Ở bước này, ta sẽ:  \n",
        "- Đếm tần suất xuất hiện của từng nhãn trong cột **`Heart Disease Status`**.  \n",
        "- Trực quan hoá bằng **biểu đồ cột** (bar chart).  \n",
        "\n",
        "Cấu hình có thể thay đổi:  \n",
        "- `LABEL_COL`: tên cột nhãn cần thống kê.  \n",
        "- `BAR_COLOR`: chọn palette hiển thị (nếu muốn đồng bộ màu sắc).  \n",
        "- `SHOW_PERCENTAGE`: hiển thị tỷ lệ % thay vì/together với số lượng tuyệt đối.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzzs3M1nJCgz"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cấu hình thống kê nhãn\n",
        "# ============================================================\n",
        "LABEL_CONFIG = {\n",
        "    \"LABEL_COL\": \"Heart Disease Status\",  # Tên cột nhãn\n",
        "    \"SHOW_PERCENTAGE\": True               # Hiển thị thêm % trên biểu đồ\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# Thống kê tần suất nhãn\n",
        "# ============================================================\n",
        "label_col = LABEL_CONFIG[\"LABEL_COL\"]\n",
        "\n",
        "label_counts = df[label_col].value_counts().reset_index()\n",
        "label_counts.columns = [label_col, \"Count\"]\n",
        "\n",
        "# Nếu cần hiển thị thêm %:\n",
        "if LABEL_CONFIG[\"SHOW_PERCENTAGE\"]:\n",
        "    total = label_counts[\"Count\"].sum()\n",
        "    label_counts[\"Percentage\"] = (label_counts[\"Count\"] / total * 100).round(2)\n",
        "\n",
        "# ============================================================\n",
        "# Biểu đồ tần suất nhãn\n",
        "# ============================================================\n",
        "fig = px.bar(\n",
        "    label_counts,\n",
        "    x=label_col,\n",
        "    y=\"Count\",\n",
        "    text=\"Count\" if not LABEL_CONFIG[\"SHOW_PERCENTAGE\"] else label_counts.apply(\n",
        "        lambda r: f\"{r['Count']} ({r['Percentage']}%)\", axis=1\n",
        "    ),\n",
        "    color=label_col,\n",
        "    title=f\"Thống kê tần suất nhãn: {label_col}\"\n",
        ")\n",
        "\n",
        "fig.update_traces(textposition=\"outside\")\n",
        "fig.update_layout(showlegend=False, xaxis_title=label_col, yaxis_title=\"Count\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuRVcDul-6T"
      },
      "source": [
        "## 1.3. Thống kê mô tả cho Numeric Columns  \n",
        "\n",
        "Ở bước này ta sẽ:  \n",
        "- Lọc ra các cột **numeric**.\n",
        "- Tạo bảng thống kê mô tả (mean, std, min, max, quartiles).  \n",
        "- Trực quan hoá phân phối bằng **histogram kèm boxplot** để phát hiện phân phối và outliers.  \n",
        "\n",
        "Cấu hình có thể thay đổi:  \n",
        "- `NUMERIC_BINS`: số lượng bins trong histogram.  \n",
        "- `SHOW_BOXPLOT`: có hiển thị boxplot phụ không.  \n",
        "- `HIST_COLOR`: màu histogram.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUacYbTbJMSj"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cấu hình thống kê numeric\n",
        "# ============================================================\n",
        "NUMERIC_CONFIG = {\n",
        "    \"NUMERIC_BINS\": 30,            # Số bins trong histogram\n",
        "    \"SHOW_BOXPLOT\": True,          # Hiển thị boxplot trên histogram\n",
        "    \"HIST_COLOR\": \"dodgerblue\"     # Màu sắc histogram\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# Lọc numeric columns\n",
        "# ============================================================\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "\n",
        "# ============================================================\n",
        "# Thống kê mô tả\n",
        "# ============================================================\n",
        "if numeric_cols:\n",
        "    display(df[numeric_cols].describe().T.style.set_caption(\"Thống kê Numeric Columns\"))\n",
        "else:\n",
        "    print(\"Không có numeric columns trong dataset.\")\n",
        "\n",
        "# ============================================================\n",
        "# Trực quan phân phối numeric\n",
        "# ============================================================\n",
        "for col in numeric_cols:\n",
        "    fig = px.histogram(\n",
        "        df,\n",
        "        x=col,\n",
        "        nbins=NUMERIC_CONFIG[\"NUMERIC_BINS\"],\n",
        "        title=f\"Phân phối {col}\",\n",
        "        marginal=\"box\" if NUMERIC_CONFIG[\"SHOW_BOXPLOT\"] else None,\n",
        "        color_discrete_sequence=[NUMERIC_CONFIG[\"HIST_COLOR\"]]\n",
        "    )\n",
        "    fig.update_layout(yaxis_title=\"Số lượng mẫu\", bargap=0.05)\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8I9pxLbmGJH"
      },
      "source": [
        "## 1.4. Thống kê mô tả cho Categorical Columns  \n",
        "\n",
        "Ở bước này ta sẽ:  \n",
        "- Lọc ra các **categorical columns** (ngoại trừ cột target `Heart Disease Status`).  \n",
        "- Tính tần suất xuất hiện của từng giá trị.  \n",
        "- Trực quan hoá bằng **pie chart** để thấy cơ cấu phân bố.  \n",
        "\n",
        "Cấu hình có thể thay đổi:  \n",
        "- `SHOW_LIMIT`: số lượng category tối đa sẽ hiển thị (giúp tránh pie chart quá rối với nhiều nhãn).  \n",
        "- `PIE_TEXTINFO`: dạng thông tin hiển thị trên chart (`\"percent\"`, `\"value\"`, `\"percent+value\"`, …).  \n",
        "- `PIE_HOLE`: hệ số donut (0 = pie chart thường, >0 = donut chart).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x_5Fg9XJOKP"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cấu hình thống kê categorical\n",
        "# ============================================================\n",
        "CATEGORICAL_CONFIG = {\n",
        "    \"SHOW_LIMIT\": 10,                   # Hiển thị tối đa bao nhiêu category (None = tất cả)\n",
        "    \"PIE_TEXTINFO\": \"percent+value\",    # Hiển thị % và số lượng\n",
        "    \"PIE_HOLE\": 0.0,                    # 0 = pie chart thường, >0 = donut chart\n",
        "    \"LABEL_COL\": \"Heart Disease Status\" # Tên cột nhãn\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# Lọc categorical columns (bỏ target)\n",
        "# ============================================================\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "categorical_cols = [col for col in categorical_cols if col != LABEL_CONFIG[\"LABEL_COL\"]]\n",
        "\n",
        "# ============================================================\n",
        "# Thống kê và trực quan hóa categorical\n",
        "# ============================================================\n",
        "if categorical_cols:\n",
        "    for col in categorical_cols:\n",
        "        counts = df[col].value_counts().reset_index()\n",
        "        counts.columns = [col, \"Count\"]\n",
        "\n",
        "        # Giới hạn số lượng category hiển thị\n",
        "        if CATEGORICAL_CONFIG[\"SHOW_LIMIT\"] is not None:\n",
        "            counts = counts.head(CATEGORICAL_CONFIG[\"SHOW_LIMIT\"])\n",
        "\n",
        "        fig = px.pie(\n",
        "            counts,\n",
        "            names=col,\n",
        "            values=\"Count\",\n",
        "            title=f\"Cơ cấu {col}\",\n",
        "            hole=CATEGORICAL_CONFIG[\"PIE_HOLE\"]\n",
        "        )\n",
        "        fig.update_traces(textinfo=CATEGORICAL_CONFIG[\"PIE_TEXTINFO\"])\n",
        "        fig.show()\n",
        "else:\n",
        "    print(\"Không có categorical columns trong dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGY1sDxGtY1h"
      },
      "source": [
        "# **2. Tiền xử lý dữ liệu**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOAFJiukN5J-"
      },
      "source": [
        "\n",
        "## 2.1. Encoding  \n",
        "\n",
        "Ở bước này ta cần:  \n",
        "1. **Định nghĩa thứ tự cho các biến categorical** (Ordinal Encoding).  \n",
        "   - Một số biến có mức độ (`Low < Medium < High`).  \n",
        "   - Một số biến boolean (`Yes/No`).  \n",
        "   - Giới tính (`Male/Female`)\n",
        "2. **Áp dụng `OrdinalEncoder`** cho các cột categorical.  \n",
        "   - Bỏ qua giá trị missing (sẽ xử lý sau bằng KNN Imputer).  \n",
        "3. **Encode target label** (`Heart Disease Status`) bằng `LabelEncoder`.  \n",
        "\n",
        "Cấu hình có thể thay đổi:  \n",
        "- `CATEGORY_MAP`: định nghĩa thứ tự cho từng nhóm biến.  \n",
        "- `TARGET_COLUMN`: tên cột target.  \n",
        "- `USE_ONEHOT_FOR`: nếu muốn OneHot thay vì Ordinal cho một số biến (vd: Gender).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1qF4xOjN4ol"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cấu hình encoding\n",
        "# ============================================================\n",
        "CATEGORY_MAP = {\n",
        "    \"Exercise Habits\": [\"Low\", \"Medium\", \"High\"],\n",
        "    \"Stress Level\": [\"Low\", \"Medium\", \"High\"],\n",
        "    \"Sugar Consumption\": [\"Low\", \"Medium\", \"High\"],\n",
        "    \"Alcohol Consumption\": [\"None\", \"Low\", \"Medium\", \"High\"],\n",
        "    \"Gender\": [\"Male\", \"Female\"],\n",
        "    # Các cột boolean khác (No/Yes)\n",
        "}\n",
        "DEFAULT_BOOLEAN = [\"No\", \"Yes\"]\n",
        "\n",
        "TARGET_COLUMN = \"Heart Disease Status\"\n",
        "\n",
        "# ============================================================\n",
        "# Build categories list cho OrdinalEncoder\n",
        "# ============================================================\n",
        "categories = []\n",
        "for col in categorical_cols:\n",
        "    if col in CATEGORY_MAP:\n",
        "        categories.append(CATEGORY_MAP[col])\n",
        "    else:\n",
        "        categories.append(DEFAULT_BOOLEAN)\n",
        "\n",
        "# ============================================================\n",
        "# Encode categorical columns bằng OrdinalEncoder\n",
        "# ============================================================\n",
        "for col, cats in zip(categorical_cols, categories):\n",
        "    ordinal_encoder = OrdinalEncoder(categories=[cats])\n",
        "    mask = df[col].notna()   # Bỏ qua missing\n",
        "    df.loc[mask, col] = ordinal_encoder.fit_transform(df.loc[mask, [col]])\n",
        "\n",
        "# ============================================================\n",
        "# Encode target bằng LabelEncoder\n",
        "# ============================================================\n",
        "label_encoder = LabelEncoder()\n",
        "df[TARGET_COLUMN] = label_encoder.fit_transform(df[TARGET_COLUMN])\n",
        "\n",
        "# ============================================================\n",
        "# Kiểm tra dữ liệu sau khi mã hoá\n",
        "# ============================================================\n",
        "display(df.head().style.set_caption(\"Dataset sau khi Encoding\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYGUKqcPoiZl"
      },
      "source": [
        "## 2.2. Chia tập Train - Test  \n",
        "\n",
        "Ở bước này ta sẽ:  \n",
        "- Tách **features (X)** và **target (y)**.  \n",
        "- Chia dữ liệu thành **train/test** để huấn luyện và đánh giá mô hình.  \n",
        "- Dùng `stratify=y` để giữ phân phối nhãn đồng đều giữa train/test.  \n",
        "\n",
        "Cấu hình có thể thay đổi:  \n",
        "- `TARGET_COLUMN`: tên cột targ_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U2W1-O_oifu"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cấu hình train-test split\n",
        "# ============================================================\n",
        "TARGET_COLUMN = \"Heart Disease Status\"\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ============================================================\n",
        "# Tách features và target\n",
        "# ============================================================\n",
        "X = df.drop(columns=[TARGET_COLUMN])\n",
        "y = df[TARGET_COLUMN]\n",
        "\n",
        "# ============================================================\n",
        "# Chia train - test\n",
        "# ============================================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Kiểm tra kết quả\n",
        "# ============================================================\n",
        "print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
        "print(\"\\n\\n\")\n",
        "display(y_train.value_counts().to_frame(\"Count\").style.set_caption(\"Label Distribution - Train\"))\n",
        "print(\"\\n\\n\")\n",
        "display(y_test.value_counts().to_frame(\"Count\").style.set_caption(\"Label Distribution - Test\"))\n",
        "\n",
        "# Xem vài dòng dữ liệu train\n",
        "display(X_train.head().style.set_caption(\"5 dòng đầu tiên - Train set\"))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Xem vài dòng dữ liệu test\n",
        "display(X_test.head().style.set_caption(\"5 dòng đầu tiên - Test set\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_lY7mBHJldS"
      },
      "source": [
        "## 2.3. Xử lý giá trị thiếu  \n",
        "\n",
        "Ở bước này ta sẽ:  \n",
        "- Dùng **KNNImputer** để điền giá trị thiếu dựa trên lân cận gần nhất.  \n",
        "- Sau khi Impute, ép kiểu các cột categorical về **int** (vì KNN sinh ra float).  \n",
        "- Kiểm tra lại xem còn missing value không.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB3NdZ_-tP6w"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# KNN Imputer cho dữ liệu thiếu\n",
        "# ============================================================\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "# Fit + transform train\n",
        "X_train_imputed = pd.DataFrame(\n",
        "    knn_imputer.fit_transform(X_train),\n",
        "    columns=X_train.columns,\n",
        "    index=X_train.index\n",
        ")\n",
        "\n",
        "# Transform test (dùng cùng imputer để tránh data leakage)\n",
        "X_test_imputed = pd.DataFrame(\n",
        "    knn_imputer.transform(X_test),\n",
        "    columns=X_test.columns,\n",
        "    index=X_test.index\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Ép categorical về int (do KNN sinh float)\n",
        "# ============================================================\n",
        "for col in categorical_cols:\n",
        "    X_train_imputed[col] = X_train_imputed[col].round().astype(\"Int64\")\n",
        "    X_test_imputed[col] = X_test_imputed[col].round().astype(\"Int64\")\n",
        "\n",
        "# ============================================================\n",
        "# Kiểm tra còn missing value không\n",
        "# ============================================================\n",
        "def check_missing(df, name=\"\"):\n",
        "    missing = df.isnull().sum()\n",
        "    missing = missing[missing > 0].sort_values(ascending=False)\n",
        "    if not missing.empty:\n",
        "        display(missing.to_frame(\"Số lượng missing\").style.set_caption(f\"Missing Values - {name}\"))\n",
        "    else:\n",
        "        print(f\"Không còn missing trong {name}\")\n",
        "\n",
        "check_missing(X_train_imputed, \"Train set\")\n",
        "check_missing(X_test_imputed, \"Test set\")\n",
        "\n",
        "# Xem vài dòng dữ liệu sau khi xử lý\n",
        "display(X_train_imputed.head().style.set_caption(\"5 dòng đầu tiên sau khi Impute - Train\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRuHjCx-mSxN"
      },
      "source": [
        "## 2.4. Scaling  \n",
        "\n",
        "- Mục đích: Chuẩn hoá các feature về cùng thang đo để mô hình học hiệu quả hơn.  \n",
        "- Kỹ thuật sử dụng mặc định: `MinMaxScaler(feature_range=(0, 1))`  \n",
        "  - Bạn có thể thay đổi cấu hình `feature_range`\n",
        "\n",
        "- Có thể thay thế bằng: `StandardScaler`, `RobustScaler`,..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDcJmHgAcyAE"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Lựa chọn scaler (có thể đổi sang StandardScaler, RobustScaler)\n",
        "# ============================================================\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# scaler = StandardScaler()\n",
        "# scaler = RobustScaler()\n",
        "\n",
        "# ============================================================\n",
        "# Fit & transform\n",
        "# ============================================================\n",
        "X_train_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X_train_imputed),\n",
        "    columns=X_train_imputed.columns,\n",
        "    index=X_train_imputed.index\n",
        ")\n",
        "\n",
        "X_test_scaled = pd.DataFrame(\n",
        "    scaler.transform(X_test_imputed),\n",
        "    columns=X_test_imputed.columns,\n",
        "    index=X_test_imputed.index\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Kiểm tra dữ liệu sau scaling\n",
        "# ============================================================\n",
        "display(X_train_scaled.head().style.set_caption(\"5 dòng đầu tiên sau khi Scaling - Train\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdB9tV1UmW0q"
      },
      "source": [
        "## 2.5. Xử lý mất cân bằng dữ liệu  \n",
        "\n",
        "- **Vấn đề:** Dữ liệu nhãn mất cân bằng khiến mô hình dễ nghiêng về lớp chiếm đa số.  \n",
        "\n",
        "- **Kỹ thuật áp dụng:**  \n",
        "  - **SMOTE (Synthetic Minority Oversampling Technique)**: sinh thêm mẫu giả lập cho lớp thiểu số.  \n",
        "\n",
        "- **Cấu hình quan trọng:**  \n",
        "  - `sampling_strategy`: tỉ lệ giữa lớp nhỏ và lớp lớn sau oversampling.  \n",
        "    - `0.5` nghĩa là lớp nhỏ = 50% lớp lớn.  \n",
        "    - `1.0` nghĩa là cân bằng tuyệt đối.  \n",
        "  - `k_neighbors`: số hàng xóm dùng để sinh mẫu mới.  \n",
        "  - `random_state`: để tái lập kết quả.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cErDgGHlC57C"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cấu hình SMOTE\n",
        "# ============================================================\n",
        "smote = SMOTE(\n",
        "    sampling_strategy=0.5,  # lớp thiểu số = 50% lớp đa số\n",
        "    k_neighbors=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Resampling (chỉ áp dụng trên tập train)\n",
        "# ============================================================\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# ============================================================\n",
        "# Kiểm tra phân phối nhãn trước & sau\n",
        "# ============================================================\n",
        "display(y_train.value_counts().to_frame(\"Count\").style.set_caption(\"Trước khi xử lý mất cân bằng\"))\n",
        "print(\"\\n\\n\")\n",
        "display(y_train_res.value_counts().to_frame(\"Count\").style.set_caption(\"Sau khi xử lý mất cân bằng\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgvaPNkCtbBA"
      },
      "source": [
        "# **3. Trích xuất & lựa chọn đặc trưng**\n",
        "\n",
        "- Trong bài toán này, ngoài việc sử dụng toàn bộ tập đặc trưng gốc, chúng ta có thể áp dụng các kỹ thuật **giảm chiều & trích xuất đặc trưng** nhằm:  \n",
        "  - Loại bỏ nhiễu và thông tin dư thừa.  \n",
        "  - Giảm chi phí tính toán, tăng tốc huấn luyện.  \n",
        "  - Tránh hiện tượng **curse of dimensionality** (khi số chiều quá lớn).  \n",
        "\n",
        "- **Kỹ thuật áp dụng:**  \n",
        "  - **PCA (Principal Component Analysis):**\n",
        "  - PCA sẽ được cấu hình như một bước tùy chọn trong pipeline huấn luyện (ở mục 4.1).  \n",
        "  - Người dùng có thể bật/tắt hoặc thay đổi số thành phần PCA (`n_components`) để đánh giá ảnh hưởng đến hiệu năng mô hình.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42Mkhbe6tdCO"
      },
      "source": [
        "# **4. Học máy**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODKGuI8imiX0"
      },
      "source": [
        "## 4.1. Kiểm tra tập train - test  \n",
        "- Sau các bước **tiền xử lý (impute, scaling, xử lý mất cân bằng)**, dữ liệu đã sẵn sàng để huấn luyện.  \n",
        "- Ở bước này, ta cần:  \n",
        "  1. Xác nhận kích thước tập train/test.  \n",
        "  2. Kiểm tra phân bố nhãn (label distribution) để đảm bảo tính cân bằng sau khi áp dụng **SMOTE**.  \n",
        "  3. Xem trước vài dòng dữ liệu để đảm bảo dữ liệu đã được xử lý đúng (không còn missing, đúng kiểu dữ liệu).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN45yTkaczqt"
      },
      "outputs": [],
      "source": [
        "# Gán lại tập train/test sau xử lý\n",
        "X_train = X_train_res\n",
        "y_train = y_train_res\n",
        "\n",
        "X_test = X_test_scaled\n",
        "y_test = y_test\n",
        "\n",
        "# Convert về DataFrame để dễ trực quan\n",
        "X_train_df = pd.DataFrame(X_train, columns=df.columns[:-1])  # bỏ cột target\n",
        "y_train_df = pd.Series(y_train, name='Heart Disease Status')\n",
        "\n",
        "X_test_df = pd.DataFrame(X_test, columns=df.columns[:-1])\n",
        "y_test_df = pd.Series(y_test, name='Heart Disease Status')\n",
        "\n",
        "# Thông tin tổng quan\n",
        "print(f\"Train samples: {len(X_train_df)}, Test samples: {len(X_test_df)}\\n\")\n",
        "\n",
        "print(\"Train label distribution:\")\n",
        "display(\n",
        "    y_train_df.value_counts()\n",
        "    .to_frame()\n",
        "    .rename(columns={'Heart Disease Status':'Count'})\n",
        ")\n",
        "\n",
        "print(\"\\nTest label distribution:\")\n",
        "display(\n",
        "    y_test_df.value_counts()\n",
        "    .to_frame()\n",
        "    .rename(columns={'Heart Disease Status':'Count'})\n",
        ")\n",
        "\n",
        "# Hiển thị vài dòng đầu\n",
        "print(\"\\nSample rows of X_train:\")\n",
        "display(X_train_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COVgRS8tpG8Z"
      },
      "source": [
        "## 4.2. Tìm tham số tối ưu cho từng mô hình\n",
        "\n",
        "- **Mục tiêu:** Trong bài toán y tế, việc dự đoán sai **có bệnh nhưng báo là không có (False Negative)** nguy hiểm hơn nhiều so với dự đoán sai chiều ngược lại.  \n",
        "  → Do đó, ta ưu tiên tối ưu **Recall** thay vì **Accuracy**.  \n",
        "\n",
        "- **Chiến lược:**  \n",
        "  - Sử dụng `GridSearchCV` để tìm tham số tối ưu cho từng mô hình.  \n",
        "  - Tích hợp **PCA** vào **Pipeline** để đồng thời tối ưu số lượng thành phần chính và siêu tham số của mô hình.  \n",
        "  - Quá trình tìm kiếm có thể bật/tắt bằng biến `SEARCH_FOR_BEST`. Mặc định, biến này để `False` nhằm tiết kiệm thời gian chạy.  \n",
        "  - Biến `SCORING_METRIC` có thể thay đổi tuỳ mục tiêu của bài toán (ví dụ `\"precision\"`, `\"accuracy\"`, `\"f1\"`, …).  \n",
        "\n",
        "### Lưu ý  \n",
        "- Kết quả tối ưu chỉ mang tính **thử nghiệm** và **tham khảo**, vì `GridSearchCV` sử dụng **cross-validation** trên tập train.  \n",
        "- Tham số tìm được chưa chắc đã mang lại hiệu quả tốt nhất trên tập test thực tế.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW4-Il0jpwdY"
      },
      "outputs": [],
      "source": [
        "SCORING_METRIC = \"recall\"\n",
        "SEARCH_FOR_BEST = False  # Đặt True để chạy GridSearchCV (có thể tốn thời gian)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eidF22_Mz6j7"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SEARCH_FOR_BEST:\n",
        "    # Pipeline PCA + RandomForest\n",
        "    pipeline = Pipeline([\n",
        "        ('pca', PCA()),\n",
        "        ('rf', RandomForestClassifier(random_state=42))\n",
        "    ])\n",
        "\n",
        "    # Grid search cả PCA và RF\n",
        "    param_grid = {\n",
        "        'pca__n_components': [0.7, 0.9, 0.99],\n",
        "        'rf__n_estimators': [50, 100, 200],\n",
        "        'rf__max_depth': [20, 50],\n",
        "        'rf__min_samples_split': [2, 5, 10],\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline,\n",
        "        param_grid,\n",
        "        cv=5,\n",
        "        scoring=SCORING_METRIC,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Best Parameters:\", grid_search.best_params_)\n",
        "    print(\"Best score:\", grid_search.best_score_)\n",
        "else:\n",
        "    print(\"Bỏ qua GridSearchCV (SEARCH_FOR_BEST=False)\")\n"
      ],
      "metadata": {
        "id": "ryXwPWOhpHDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR6jCONLz9kA"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Logistic Regression\n",
        "if SEARCH_FOR_BEST:\n",
        "    # Pipeline PCA + Logistic Regression\n",
        "    pipeline = Pipeline([\n",
        "        ('pca', PCA()),\n",
        "        ('lr', LogisticRegression(max_iter=3000, random_state=42))\n",
        "    ])\n",
        "\n",
        "    # Grid search cả PCA và Logistic Regression\n",
        "    param_grid = {\n",
        "        'pca__n_components': [0.7, 0.8, 0.9, 0.99],\n",
        "        'lr__C': [0.01, 0.1, 1, 10, 100],\n",
        "        'lr__penalty': [\"l1\", \"l2\"],\n",
        "        'lr__solver': [\"liblinear\", \"saga\"],  # tương thích với cả l1 và l2\n",
        "        'lr__class_weight': [None, \"balanced\"]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline,\n",
        "        param_grid,\n",
        "        cv=5,\n",
        "        scoring=SCORING_METRIC,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Best Parameters:\", grid_search.best_params_)\n",
        "    print(\"Best score:\", grid_search.best_score_)\n",
        "else:\n",
        "    print(\"Bỏ qua GridSearchCV (SEARCH_FOR_BEST=False)\")"
      ],
      "metadata": {
        "id": "lxJoFlRHpbZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN1ImA5X0Aoy"
      },
      "source": [
        "### k-NN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SEARCH_FOR_BEST:\n",
        "    # Pipeline PCA + KNN\n",
        "    pipeline = Pipeline([\n",
        "        ('pca', PCA()),\n",
        "        ('knn', KNeighborsClassifier())\n",
        "    ])\n",
        "\n",
        "    # Grid search PCA + KNN\n",
        "    param_grid = {\n",
        "        'pca__n_components': [0.7, 0.8, 0.9, 0.99],\n",
        "        'knn__n_neighbors': [3, 5, 7, 9, 11, 15],\n",
        "        'knn__weights': [\"uniform\", \"distance\"],\n",
        "        'knn__p': [1, 2],\n",
        "        'knn__metric': [\"minkowski\"]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline,\n",
        "        param_grid,\n",
        "        cv=5,\n",
        "        scoring=SCORING_METRIC,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Best Parameters:\", grid_search.best_params_)\n",
        "    print(\"Best score:\", grid_search.best_score_)\n",
        "else:\n",
        "    print(\"Bỏ qua GridSearchCV (SEARCH_FOR_BEST=False)\")"
      ],
      "metadata": {
        "id": "xJUGRPqYpbev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piv22n6L0Cdh"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTPMfGvKpbnW"
      },
      "outputs": [],
      "source": [
        "### SVM\n",
        "if SEARCH_FOR_BEST:\n",
        "    # Pipeline PCA + SVM\n",
        "    pipeline = Pipeline([\n",
        "        ('pca', PCA()),\n",
        "        ('svc', SVC(random_state=42, max_iter=5000))\n",
        "    ])\n",
        "\n",
        "    # Grid search PCA + SVM\n",
        "    param_grid = {\n",
        "        'pca__n_components': [0.7, 0.8, 0.9, 0.99],\n",
        "        'svc__C': [0.1, 1, 10, 100],\n",
        "        'svc__kernel': [\"linear\", \"rbf\", \"poly\"],\n",
        "        'svc__class_weight': [None, \"balanced\"],\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline,\n",
        "        param_grid,\n",
        "        cv=5,\n",
        "        scoring=SCORING_METRIC,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Best Parameters:\", grid_search.best_params_)\n",
        "    print(\"Best score:\", grid_search.best_score_)\n",
        "else:\n",
        "    print(\"Bỏ qua GridSearchCV (SEARCH_FOR_BEST=False)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4liQkPKe0EAe"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtNZcqcUpbuf"
      },
      "outputs": [],
      "source": [
        "if SEARCH_FOR_BEST:\n",
        "    # Pipeline PCA + Naive Bayes\n",
        "    pipeline = Pipeline([\n",
        "        ('pca', PCA()),\n",
        "        ('nb', GaussianNB())\n",
        "    ])\n",
        "\n",
        "    # Grid search PCA + NB\n",
        "    param_grid = {\n",
        "        'pca__n_components': [0.7, 0.8, 0.9, 0.99],\n",
        "        'nb__var_smoothing': [1e-09, 1e-06, 1e-03, 1]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline,\n",
        "        param_grid,\n",
        "        cv=5,\n",
        "        scoring=SCORING_METRIC,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Best Parameters:\", grid_search.best_params_)\n",
        "    print(\"Best score:\", grid_search.best_score_)\n",
        "else:\n",
        "    print(\"Bỏ qua GridSearchCV (SEARCH_FOR_BEST=False)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfdM8Lsg0HQb"
      },
      "source": [
        "### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAkgy9rqpbzv"
      },
      "outputs": [],
      "source": [
        "### Decision Tree\n",
        "if SEARCH_FOR_BEST:\n",
        "    # Pipeline PCA + Decision Tree\n",
        "    pipeline = Pipeline([\n",
        "        ('pca', PCA()),\n",
        "        ('dt', DecisionTreeClassifier(random_state=42))\n",
        "    ])\n",
        "\n",
        "    # Grid search PCA + Decision Tree\n",
        "    param_grid = {\n",
        "        'pca__n_components': [0.7, 0.8, 0.9, 0.99],\n",
        "        'dt__criterion': [\"gini\", \"entropy\"],\n",
        "        'dt__max_depth': [None, 5, 10, 20],\n",
        "        'dt__min_samples_split': [2, 5],\n",
        "        'dt__class_weight': [None, \"balanced\"],\n",
        "        'dt__ccp_alpha': [0.0, 0.01]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline,\n",
        "        param_grid,\n",
        "        cv=5,\n",
        "        scoring=SCORING_METRIC,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Best Parameters:\", grid_search.best_params_)\n",
        "    print(\"Best score:\", grid_search.best_score_)\n",
        "else:\n",
        "    print(\"Bỏ qua GridSearchCV (SEARCH_FOR_BEST=False)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbLpIrT2mxy2"
      },
      "source": [
        "## 4.3. Huấn luyện các mô hình với PCA  \n",
        "\n",
        "- Sau khi tiền xử lý, ta huấn luyện nhiều mô hình học máy trên dữ liệu đã giảm chiều bằng **PCA**.  \n",
        "- Các mức phương sai được giữ lại: **70%, 80%, 90%, 99%**.  \n",
        "- Các mô hình sử dụng:  \n",
        "  - Random Forest  \n",
        "  - SVM  \n",
        "  - Logistic Regression  \n",
        "  - k-NN  \n",
        "  - Naive Bayes  \n",
        "  - Decision Tree  \n",
        "\n",
        "### Quy trình  \n",
        "1. Xây dựng **Pipeline** gồm PCA và mô hình.  \n",
        "2. Huấn luyện trên tập **train** (đã cân bằng bằng SMOTE).  \n",
        "3. Dự đoán trên tập **test** (không áp dụng SMOTE).  \n",
        "4. Đánh giá mô hình theo các metric:  \n",
        "   - **Accuracy**  \n",
        "   - **Precision**  \n",
        "   - **Recall**  \n",
        "   - **F1-score**  \n",
        "5. Hiển thị **Confusion Matrix** cho từng mô hình để quan sát chi tiết.  \n",
        "\n",
        "### Lưu ý  \n",
        "- Các cấu hình (siêu tham số) trong phần này được thiết lập thủ công.  \n",
        "- Có thể điều chỉnh lại cấu hình nếu muốn kiểm chứng hoặc cải thiện hiệu năng.  \n",
        "- Kết quả tối ưu ở **mục 4.2** chỉ mang tính **tham khảo**, vì được tìm bằng **GridSearchCV** trên tập train và chưa chắc đã đạt hiệu quả tốt nhất trên tập test thực tế.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg3Pq_BimrRT"
      },
      "outputs": [],
      "source": [
        "# Định nghĩa danh sách mô hình\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(\n",
        "        max_depth=20, min_samples_split=2, n_estimators=200, class_weight=\"balanced\"\n",
        "    ),\n",
        "    \"SVM\": SVC(C=10, kernel='rbf', class_weight=\"balanced\"),\n",
        "    \"Logistic Regression\": LogisticRegression(\n",
        "        C=0.1, class_weight=\"balanced\", penalty=\"l1\", solver=\"liblinear\"\n",
        "    ),\n",
        "    \"KNN\": KNeighborsClassifier(\n",
        "        metric=\"minkowski\", n_neighbors=3, p=1, weights='distance'\n",
        "    ),\n",
        "    \"Naive Bayes\": GaussianNB(var_smoothing=1e-09),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(\n",
        "        ccp_alpha=0.0, criterion='gini', max_depth=5,\n",
        "        min_samples_split=2, class_weight=\"balanced\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Các mức PCA giữ lại\n",
        "pca_variances = [0.7, 0.8, 0.9, 0.99]\n",
        "\n",
        "# Lưu kết quả\n",
        "results = []\n",
        "\n",
        "# Huấn luyện & đánh giá\n",
        "for pca_var in pca_variances:\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\n=== Training {model_name} with PCA={int(pca_var*100)}% ===\")\n",
        "\n",
        "        # Pipeline PCA + Model\n",
        "        pipeline = Pipeline([\n",
        "            ('pca', PCA(n_components=pca_var)),\n",
        "            ('clf', model)\n",
        "        ])\n",
        "\n",
        "        # Train\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Predict trên tập test\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "\n",
        "        # Đánh giá\n",
        "        acc  = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "        rec  = recall_score(y_test, y_pred, zero_division=0)\n",
        "        f1   = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "        results.append({\n",
        "            \"Model\": model_name,\n",
        "            \"PCA\": int(pca_var*100),\n",
        "            \"Accuracy\": acc,\n",
        "            \"Precision\": prec,\n",
        "            \"Recall\": rec,\n",
        "            \"F1-score\": f1\n",
        "        })\n",
        "\n",
        "        # Hiển thị Confusion Matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        cm_df = pd.DataFrame(\n",
        "            cm,\n",
        "            index=[f\"Actual {cls}\" for cls in label_encoder.classes_],\n",
        "            columns=[f\"Pred {cls}\" for cls in label_encoder.classes_]\n",
        "        )\n",
        "        display(cm_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKPu1Bfam6TY"
      },
      "source": [
        "## 4.4. So sánh và đánh giá\n",
        "\n",
        "Sau khi huấn luyện các mô hình với các mức PCA khác nhau, ta tiến hành:  \n",
        "  1. Tổng hợp kết quả đánh giá của tất cả mô hình.  \n",
        "  2. Xác định mô hình tốt nhất theo từng metric (**Accuracy, Precision, Recall, F1-score**).  \n",
        "  3. Trực quan hóa so sánh các mô hình dưới từng mức PCA để quan sát sự khác biệt.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FB1hHQPzm6b9"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 1. Tổng hợp kết quả\n",
        "# ======================\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"=== Comparison Table ===\")\n",
        "display(results_df.style.hide(axis=\"index\"))\n",
        "\n",
        "# ======================\n",
        "# 2. Mô hình tốt nhất theo từng metric\n",
        "# ======================\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "best_summary = []\n",
        "\n",
        "for metric in metrics:\n",
        "    idx = results_df[metric].idxmax()\n",
        "    best_row = results_df.loc[idx]\n",
        "    best_summary.append({\n",
        "        'Metric': metric,\n",
        "        'Best Model': best_row['Model'],\n",
        "        'Best PCA': best_row['PCA'],\n",
        "        'Best Score': best_row[metric]\n",
        "    })\n",
        "\n",
        "print(\"\\n=== Best Model per Metric ===\")\n",
        "display(pd.DataFrame(best_summary).style.hide(axis=\"index\"))\n",
        "\n",
        "# ======================\n",
        "# 3. Trực quan hóa kết quả theo từng mức PCA\n",
        "# ======================\n",
        "pca_values = sorted(results_df['PCA'].unique())\n",
        "bar_width = 0.2\n",
        "\n",
        "for pca in pca_values:\n",
        "    df_pca = results_df[results_df['PCA'] == pca]\n",
        "\n",
        "    models_compare = df_pca['Model'].values\n",
        "    accuracy = df_pca['Accuracy'].values\n",
        "    precision = df_pca['Precision'].values\n",
        "    recall = df_pca['Recall'].values\n",
        "    f1_score_vals = df_pca['F1-score'].values\n",
        "\n",
        "    x = np.arange(len(models_compare))\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.bar(x - 1.5*bar_width, accuracy, bar_width, label=\"Accuracy\", color=\"blue\")\n",
        "    ax.bar(x - 0.5*bar_width, precision, bar_width, label=\"Precision\", color=\"green\")\n",
        "    ax.bar(x + 0.5*bar_width, recall, bar_width, label=\"Recall\", color=\"orange\")\n",
        "    ax.bar(x + 1.5*bar_width, f1_score_vals, bar_width, label=\"F1-score\", color=\"red\")\n",
        "\n",
        "    ax.set_xlabel(\"Models\", fontsize=12)\n",
        "    ax.set_ylabel(\"Score\", fontsize=12)\n",
        "    ax.set_title(f\"Model Comparison for PCA = {pca}%\", fontsize=14)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(models_compare, rotation=15)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.legend()\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Kết luận**\n",
        "\n",
        "Với bộ dữ liệu **Heart Disease** từ Kaggle, nhóm đã xây dựng và thử nghiệm một pipeline phân loại bệnh tim với nhiều mô hình khác nhau, kết hợp PCA và các tiêu chí đánh giá phổ biến. Kết quả cho thấy từng mô hình có ưu thế riêng, đồng thời khẳng định **Recall** là chỉ số quan trọng trong bối cảnh y tế nhằm hạn chế bỏ sót ca bệnh.  \n",
        "\n",
        "Tham khảo:  \n",
        "- [Heart Disease Prediction with 83.8% Accuracy](https://www.kaggle.com/code/hossainhedayati/heart-disease-prediction-with-83-8-accuracy)\n"
      ],
      "metadata": {
        "id": "LbX_zZkKoA6q"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}